\section{Feature selection method}
\label{sec.method}


In this section,
we describe
our method of feature selection.

Let 
$\spaceX \subset \Rd$
and 
$\spaceY \subset \R^{e}$
represent sample spaces.
Let 
$X$
and 
$Y$
be random variables taking values in 
$\spaceX$
and 
$\spaceY$
respectively.
We interpret $Y$ as 
the target of a
prediction / classification
task,
and
we interpret $X$ as
a vector of features to use in this
prediction / classification
task.

Given $n$ samples
\begin{equation*}
	(x_1, y_1), 
	\dots,
	(x_n, y_n), 
\end{equation*}
from the joint distribution $\Prob_{X, Y}$, 
a permutation $\sigma \in S_n$,
a real valued function $f: \spaceX \times \spaceY \rightarrow \R$,
and
a $d$-dimensional vector $\projection \in \Rd$,
we write
\begin{equation}
	\begin{split}
		\jointeval[f, \projection] &= \frac{1}{n} \sum_{i=1}^{n} f(\projection \hadamard x_i, y_i),
		\\
		\prodeval[f, \projection] &= \frac{1}{n} \sum_{i=1}^{n} \exp\left(f(\projection \hadamard x_{\sigma(i)}, y_i)\right),
	\end{split}
\end{equation}
where
$\projection \hadamard x_i$ 
is the Hadamard product of 
$\projection$ and $x_i$. 
We use 
$\jointeval[f, \projection]$ 
to approximate 
$\Expectation_{\Prob_{XY}}[f(\projection \hadamard X, Y)]$,
and we use
$\prodeval[f, \projection]$ 
to approximate 
$\Expectation_{\Prob_{X} \otimes \Prob_{Y}}[\exp\left(f(\projection \hadamard X, Y)\right)]$,

Let 
$f_\theta$, $\theta \in \Theta$
be a famility of measurable functions
\begin{equation*}
	f_\theta: \spaceX \times \spaceY \rightarrow \R
\end{equation*}
parametrised by the parameter $\theta \in \Theta$ of a neural network.
Let 
$\projection \in \Rd$
be a $d$-dimensional vector. 
We define
\begin{equation}
	\donskervaradhanloss(\theta, \projection) = - \jointeval[f_\theta, \projection] + \log \left(\prodeval[f_\theta, \projection]\right)
\end{equation}
and, recalling Section \ref{sec.mine},
we consider 
$\donskervaradhanloss ( \cdot, p)$ 
as an approximation of the negative of the mutual information of $p \odot X$ and $Y$. 
Moreover, 
for 
non-negative real coefficients $c_1$, $c_2$, $a$
we define
\begin{equation}
	\lossfun(\theta, p, c_1, c_2, a)
 \,\,
	=  
 \,\,
	\donskervaradhanloss(\theta, \projection)
		 + 
		c_1 \lonenorm[\frac{\projection}{\ltwonorm[\projection]}]
		 + 
		c_2 \left( \ltwonorm[\projection] - a \right)\squared,
\end{equation}
where 
$\lonenorm$ denotes $\Lone$-norm 
and 
$\ltwonorm$ denotes $\Ltwo$-norm.

The function $\lossfun$ is the loss function. 
It consists of three terms.
The first term 
$\donskervaradhanloss(\theta, \projection)$
is the discretisation of the functional that appears 
in the Donsker-Varadhan representation of 
the Kullback-Leibler divergence.
It approximates the negative mutual information 
between the target and the $\projection$-weighted features.

The second term 
$
\lonenorm[\frac{\projection}{\ltwonorm[\projection]}]
$
is a regularisation term on the
weights $\projection \in \Rd$.
It induces sparsity 
by pushing to zero the weights of non-relevant features.

Finally,
the third term
$
\left( \ltwonorm[\projection] - a \right)\squared
$
controls the euclidean norm of the 
weights $\projection \in \Rd$
by penalising the square of the difference between
said norm 
and
the target norm $a$.
This is meant
to prevent 
the weights of relevant features 
from diverging.


Our feature selection method
consists in
finding a minimiser 
$\hat{\theta}$ of 
\begin{equation*}
\theta \longmapsto \donskervaradhanloss(\theta, \ones),
\end{equation*}
where $\ones = (1, \dots, 1) \in \Rd$,
and then using this $\hat{\theta}$
as the initialisation of the gradient descent 
for the minimisation of 
\begin{equation*}
	\theta, \projection \longmapsto \lossfun\left(\theta, \projection, c_1, c_2, \sqrt{d}\right).
\end{equation*}
We stop this gradient descent 
when 
the estimated mutual information 
between 
the weighted features
and
the targets
becomes smaller than 
the mutual information 
that corresponds to the minimiser 
$\hat{\theta}$.
After the gradient descent has stopped,
we select the features that correspond to non-null weights,
i.e. to non-null entries of $\projection$.
More precisely, our method is described in Algorithm \ref{algo.minerva}.

The architecture 
of 
the neural network 
used in 
the parametrisation  of the test functions 
$f_\theta$
is represented in 
Figure
\ref{fig.networkarchitecture}.

We implement our MINE-based feature selection in the 
\weblink{https://pypi.org/}{pypi}-package 
\weblink{https://github.com/transferwise/minerva}{minerva}.\footnote{
	See
	\url{https://github.com/transferwise/minerva}
}
Its usage is demonstrated 
with 
\weblink{https://github.com/transferwise/minerva/tree/main/notebooks}{Jupyter notebooks}.\footnote{
	See \url{https://github.com/transferwise/minerva/tree/main/notebooks}
}

\begin{algorithm}
	\caption{Mutual Information Neural Estimation Regularized Vetting Algorithm}
	\label{algo.minerva}
	\begin{algorithmic}[1]
		\REQUIRE
		random variables
		$X\in \spaceX$,
		$Y\in \spaceY$,
		hyperparameters
		$r>0$, $c_1\geq 0$, $c_2\geq 0$.
		\STATE $\theta \leftarrow$ initialise network parameters
		\REPEAT
		\STATE Draw $n$ samples $(x_1, y_1), \dots, (x_n, y_n)$ from the joint distribution $\Prob_{XY}$
		\STATE Sample shuffling permutation $\sigma$ from $S_n$
		\STATE Update $\theta \leftarrow \theta - r\gradient_{\theta}\donskervaradhanloss(\theta, \ones)$%, where $\ones = (1, \dots, 1)$.
		\UNTIL{ convergence}
		\STATE Initialise $\varphi \leftarrow \theta$, $\projection \leftarrow \ones$.
		\REPEAT
		\STATE Draw $n$ samples $(x_1, y_1), \dots, (x_n, y_n)$ from the joint distribution $\Prob_{XY}$
		\STATE Sample shuffling permutation $\sigma$ from $S_n$
		\STATE Update $\varphi \leftarrow \varphi - r \gradient_{\varphi}\lossfun(\varphi, \projection, c_1, c_2, \sqrt{d})$
		\STATE Update $\projection \leftarrow \projection - r \gradient_{\projection}\lossfun(\varphi, \projection, c_1, c_2, \sqrt{d})$
		\UNTIL{ convergence}
		\RETURN $\left\lbrace i: \abs{\projection_i} > 0\right\rbrace$
	\end{algorithmic}
\end{algorithm}

\begin{figure}
	\caption{Neural network architecture}
	\label{fig.networkarchitecture}
	\includegraphics[width=.9\textwidth]{architecture.png}
\end{figure}





