\section{Experiments}
In this section,
we present the results of our numerical experiments.
Our experiments are based
either 
on real-world datasets
or on synthetic data.

With real-world datasets,
we evaluate
feature selection methods
by looking at 
how
the out-of-sample accuracy of the prediction / classification
improves with the reduced dimensionality of the selected features
compared to the full datasets.

With synthetic data, 
out 
of the $d$-features to select from,
we know the subset 
$t \subset \lbrace 1, \dots, d\rbrace$
that the target depends on,
and thus
we can evaluate feature selection methods
by reconciling their selection with $t$.
More precisely,
let 
$s$ 
be a subset of 
$\lbrace1, \dots, d\rbrace$.
We say that 
the selection of the features 
$s$ is exact if $s=t$,
and it is non-exact otherwise. 
If the selection is non-exact,
either 
$t \not \subset s$
or
$s \supsetneq t$.
In the former case, 
we say that the non-exact selection is of type I;
in the latter case,
we say that the non-exact selection is of type II. 
Non-exact selections of type I 
compromise the downstream prediction task 
because 
they subtract information relevant for the prediction.
Non-exact selections of type II
might not reduce the dimensionality of the problem,
but they do not compromise downstream tasks. 


\subsection{Experiment 1}
We study the phenomenon
whereby
a target $Y$ depends
on whether
two independent discrete random variables 
$X_{k_0}$ 
and
$X_{k_1}$
are equal or not.

On the one hand, 
this sort of dependence is relevant in practice.
Assume, 
for example,
that 
you are dealing with a data set recording 
international money transfers.
This data set will have 
one column $X_{k_0}$ recording 
the currency of the country from which the transfer is sent,
and another column $X_{k_1}$ recording
the currency of the country to which the transfer is sent.
The distribution of your data will
depend on whether the transfer is multicurrency or not, 
namely on whether $X_{k_0} = X_{k_1}$.
% This is the real-world example 
% that we have in mind when experimenting with synthetic data in this section.

On the other hand,
this sort of dependence
is not well captured by
existing feature selection filters. 
We demonstrate this in a first small example.
Then,
we assess the performance of our feature selection filter 
on a synthetic dataset that 
is meant to be representative of a common regression learning tasks,
and that embeds the dependence that 
existing feature selection filters cannot capture.

\subsubsection{Experiment 1.A}
Let
$d$
be a positive integer,
and 
let 
$m > 2$
be  a positive integer larger than $2$.
For
$i = 1, \dots, d$
let
$X_i$
be a random  positive integer
smaller that or equal to $m$.
The random variables
$X_1, \dots, X_d$
are assumed independent and identically distributed.
Fix 
two integers
$ 1 \leq k_0 < k_1 \leq d$
and 
define
\begin{equation}
	\label{eq.exp1Y}
	Y = \one\left\lbrace
	X_{k_0} = X_{k_1}
	\right
	\rbrace
	=
	\begin{cases}
		1 & \text{ if } X_{k_0} = X_{k_1}
		\\
		0 & \text{ otherwise}.
	\end{cases}
\end{equation}

We consider the task of predicting $Y$ from the vector 
$(X_1, \dots, X_d)$,
and 
we want to select 
from this vector 
the features that are relevant for the prediction.

In this context,
feature selection methods
that rely on a metric $h(X_i, Y)$ of dependence between 
each feature $X_i$ and the target $Y$ are bound to fail.
This is explained in Lemma \ref{lemma.experiment1}:
the pair-wise assessment of $(X_i, Y)$ 
cannot possibly produce an exact selection
because $Y$ is independent from every $X_i$. 
It is only by considering 
the ensemble of features $X_1, \dots, X_d$ 
that we can produce an exact selection.

We confirm that in our numerical experiments. 
We test 
our MINE-based feature selection method 
against 
two benchmarks.

The first benchmark is 
\weblink{https://scikit-learn.org/stable/modules/generated/sklearn.feature\_selection.mutual\_info\_classif.html}{sklearn.feature\_selection.mutual\_info\_classif}.\footnote{
See
\url{https://scikit-learn.org/stable/modules/generated/sklearn.feature\_selection.mutual\_info\_classif.html}
}
This method estimates the mutual information 
$I(X_i; Y)$ for all $i = 1, \dots, d$
and 
it selects those features $k$ such that 
$I(X_k; Y) > \epsilon$
for a given threhsold $\epsilon \geq 0$. 
The estimation of $I(X_i; Y)$ is based on the KSG estimator, 
introduced in 
\cite{KSG04est}.

The second benchmark is
HSIC Lasso,
see 
\cite{YJSXS14hig}.
This method 
selects features $i_1, \dots, i_k$
that correspond to non-null entries of the 
maximisers of 
\begin{equation*}
	\beta \mapsto 
	\sum_{i=1}^{d} \beta_i h(X_i, Y)
	-\half
	\sum_{i,j = 1}^{d} \beta_i \beta_j h(X_i, X_j),
\end{equation*}
where $h$ is the Hilbert-Schmidt independence criterion 
introduced in 
\cite{GBSS05mea}.
We use the implementation of HSIC Lasso
given in
\weblink{https://pypi.org/project/pyHSICLasso/}{pyHSICLasso}.\footnote{
See \url{https://pypi.org/project/pyHSICLasso/}
}

Data and code for our experiments are available 
at
\weblink{https://github.com/transferwise/minerva/tree/main/experiments/experiment\_1/categorical\_only}{experiment\_1/categorical\_only}.\footnote{
	See
\url{https://github.com/transferwise/minerva/tree/main/experiments}. 
}
More precisely, 
the synthetic dataset that we use is
 [where?]
and 
the script used for training minerva is
\weblink{https://github.com/transferwise/minerva/blob/main/experiments/experiment\_1/categorical\_only/minervarunner.py}{minervarunner.py}.\footnote{
	See
\url{https://github.com/transferwise/minerva/blob/main/experiments/experiment\_1/categorical\_only/minervarunner.py}
}
The benchmark methods of feature selection 
were
executed
in the script
\weblink{https://github.com/transferwise/minerva/blob/main/experiments/experiment\_1/categorical\_only/benchmarks.py}{benchmarks.py}.\footnote{
	See
\url{https://github.com/transferwise/minerva/blob/main/experiments/experiment\_1/categorical\_only/benchmarks.py}
}


Table
\ref{tab.experiment1A}
summarises our findings. 
As expected, 
neither 
\weblink{https://scikit-learn.org/stable/modules/generated/sklearn.feature\_selection.mutual\_info\_classif.html}{sklearn.feature\_selection.mutual\_info\_classif}
nor
\weblink{https://pypi.org/project/pyHSICLasso/}{pyHSICLasso}
were able to complete an exact selection. 
Their selections are non-exact of type II. 
Notice moreover that if we force 
\weblink{https://pypi.org/project/pyHSICLasso/}{pyHSICLasso}
to select two features only,
the selection is non-exact of type I.
Instead,
Minerva
was able to complete an exact selection.

\begin{table}
	\centering
{\rowcolors{2}{gray!20!white!50}{gray!10!white!40}
	\begin{tabular}{l|l|l|l}
		\textbf{method}
		&
		\textbf{selected}
		&
		\textbf{expected}
		&
		\textbf{evaluation}
		\\
		\hline
		mutual\_info\_classif
		&
		1, \dots, 30
		& 
		3, 8
		&
		non-exact type II
		\\
		pyHSICLasso
		&
		1, \dots, 30
		& 
		3, 8
		&
		non-exact type II
		\\
		minerva
		& 
		3, 8
		&
		3, 8
		&
		exact
	\end{tabular}
}
\caption{Experiment 1.A - Comparison of three different feature selection methods.}
\label{tab.experiment1A}
\end{table}






\begin{lemma}
	\label{lemma.experiment1}
	Let $m>2$ be a positive integer. 
	Let
	$X_1, \dots, X_d$
	be independent identically distributed
	with 
	$\Prob(X_1 = n) = 1 / m$
	for $n=1, \dots, m$. 
	Let 
	$k_0$ and $k_1$ 
	be two distinct positive integers 
	smaller than or equal to $d$,
	and let $Y$
	be as in equation \eqref{eq.exp1Y}.
	Then,
	for all $i=1, \dots, d$
	\begin{equation}
		\label{eq.exp1pairwisemi}
		I(X_i; Y) = 0,
	\end{equation}
	namely $X_i$ and $Y$ are independent. 
	Moreover,
	\begin{equation}
		\label{eq.exp1mi}
		I(X_{k_0}, X_{k_1}; Y) 
		=
			\frac{m-1}{m} \log\left(\frac{m}{m-1}\right)
			+
			\frac{1}{m}\log m
			,
	\end{equation}
	and $I(X_{k_0}; Y \lvert X_{k_1}) = I(X_{k_1}; Y \lvert X_{k_0})  = I(X_{k_0}, X_{k_1} ; Y)$.
\end{lemma}




\subsubsection{Experiment 1.B}

Let 
$d_1, d_2$
be positive integers.
Let
$X_1, \dots, X_{d_1}$
be i.i.d random variables
such that
$\Prob(X_1 = k) = 1/m$
for $k=1, \dots, m$,
for some positive integer 
$m > 1$.
Let 
$X_{d_1 + 1}, \dots, X_{d_1 + d_2}$
be i.i.d random variables with uniform distribution on the unit interval. 
It is assumed that 
$X_1, \dots, X_{d_1}$
and
$X_{d_1 + 1}, \dots, X_{d_1 + d_2}$
are independent. 
Let 
$k_0, k_1$
be distinct positive integers 
smaller than or equal to
$m$.
Let 
$n < d_2$
and 
let
$d_1 < j_0 <  \dots < j_n \leq d_1 + d_2$
and
$d_1 < i_0 <  \dots < i_n \leq d_1 + d_2$.
We define
\begin{equation}
	\label{eq.exp2target}
	Y 
	=
	\begin{cases}
		\sum_{\ell = 1}^{\ell=n} \alpha_\ell \sin\left(2\pi X_{j_\ell}\right)  
		& \text{ if } X_{k_0} = X_{k_1}
		\\
		\sum_{\ell = 1}^{\ell=n} \beta_\ell \cos\left(2\pi X_{i_\ell}\right)  
		& \text{ otherwise}.
	\end{cases}
\end{equation}



We consider the task of predicting $Y$ from the vector 
$(X_1, \dots, X_{d_1}, X_{d_1 + 1}, \dots, X_{d_1 + d_2})$,
and 
we want to select 
from this vector 
the features that are relevant for the prediction.
This setup is a combination of a 
straightforward feature selection setup 
where a target depends non-linearly on a subset of features,
and 
the sort of dependence utilised in Experiment 1.A.
Namely,
we assume that 
there are two continuous non-linear functions $f_1$ and $f_2$,
and
the target is a transformation through $f_1$
of some of the continuous features if two discrete  variables happen to be equal,
and it is a transformation through $f_2$
of some other continuous features if those two discrete variables are not equal. 

We test 
Minerva
against two benchmark filters, 
and one bechmark wrapper.

The first benchmark filter is 
\weblink{https://scikit-learn.org/stable/modules/generated/sklearn.feature\_selection.mutual\_info\_regression.html}{sklearn.feature\_selection.mutual\_info\_regression}.\footnote{
See
\url{https://scikit-learn.org/stable/modules/generated/sklearn.feature\_selection.mutual\_info\_regression.html}
}
The second benchmark filter is
HSIC Lasso
as implemented in 
\weblink{https://pypi.org/project/pyHSICLasso/}{pyHSICLasso}.\footnote{
See \url{https://pypi.org/project/pyHSICLasso/}
}
The benchmark wrapper is Boruta,
as implmented in
\weblink{https://pypi.org/project/arfs/}{arfs}.\footnote{
See
\url{https://pypi.org/project/arfs/}
}


Data and code for our experiments are available 
at
\weblink{https://github.com/transferwise/minerva/tree/main/experiments/experiment\_1/full}{experiment\_1/full}.\footnote{
	See
\url{https://github.com/transferwise/minerva/tree/main/experiments}
}
More precisely, 
the synthetic dataset that we use is
[where?]
and 
the script used for training minerva is
\weblink{https://github.com/transferwise/minerva/blob/main/experiments/experiment\_1/full/minervarunner.py}{minervarunner.py}.\footnote{
	See
\url{https://github.com/transferwise/minerva/blob/main/experiments/experiment\_1/full/minervarunner.py}
}
The training history of the 
selection weights
in the vector $p$
is available at
\weblink{https://github.com/transferwise/minerva/blob/main/experiments/experiment\_1/full/data/run4/weight\_history\_7\_segment0.csv}{weight\_history.csv}.\footnote{
	See
\url{https://github.com/transferwise/minerva/blob/main/experiments/experiment\_1/full/data/run4/weight\_history\_7\_segment0.csv}
}
The benchmark methods of feature selection 
were
executed
in the script
\weblink{https://github.com/transferwise/minerva/blob/main/experiments/experiment\_1/full/benchmarks.py}{benchmarks.py}.\footnote{
	See
\url{https://github.com/transferwise/minerva/blob/main/experiments/experiment\_1/full/benchmarks.py}
}



Table
\ref{tab.experiment1B}
summarises our findings. 
Minerva is the only method capable of completing an exact selection.
Sklearn's mutual information, 
HSIC Lasso,
and 
Boruta
perform a non-exact selection of type I.
This is reflected in the prediction of the target given the selected features:
out-of-sample accuracy of a gradient boosting model
trained on Minerva's selection
decisevely outperfoms
the accuracies of the same model trained on
the features selected by the other methods.
See 
Table \ref{tab.experiment1Baccuracy}
and
the notebook
\weblink{https://github.com/transferwise/minerva/blob/main/experiments/experiment\_1/full/accuracy.ipynb}{accuracy.ipynb}






\begin{table}
	\centering
{\rowcolors{2}{gray!20!white!50}{gray!10!white!40}
	\begin{tabular}{l|l|l|l}
		\textbf{method}
		&
		\textbf{selected}
		&
		\textbf{expected}
		&
		\textbf{evaluation}
		\\
		\hline
		mutual\_info\_regression
		&
		\tiny{
		14, 18, 19, 20, 23, 25, 28, 31, 34, 38
	}
		& 
		\tiny{
		6, 8, 14, 18, 19, 20, 23, 24, 28, 31
	}
		&
		non-exact type I
		\\
		pyHSICLasso
		&
		\tiny{
		4, 11, 14, 18, 19, 20, 23, 24, 28, 31
	}
		& 
		\tiny{
		6, 8, 14, 18, 19, 20, 23, 24, 28, 31
	}
		&
		non-exact type I
		\\
		Boruta
		&
		\tiny{
		14, 18, 19, 20, 23, 24, 28, 31
	}
		& 
		\tiny{
		6, 8, 14, 18, 19, 20, 23, 24, 28, 31
	}
		&
		non-exact type I
		\\
		minerva
		& 
		\tiny{
		6, 8, 14, 18, 19, 20, 23, 24, 28, 31
	}
		&
		\tiny{
		6, 8, 14, 18, 19, 20, 23, 24, 28, 31
	}
		&
		exact
	\end{tabular}
}
\caption{Experiment 1.B - Comparison of four different feature selection methods.}
\label{tab.experiment1B}
\end{table}


\begin{table}
	\centering
{\rowcolors{2}{gray!20!white!50}{gray!10!white!40}
	\begin{tabular}{l|c|r|r}
		\textbf{method}
		&
		\textbf{number of features}
		&
		\textbf{in-sample R2}
		&
		\textbf{out-of--sample R2}
		\\
		\hline
		no selection (use all features)
		&
		40
		&
		0.8615
		&
		0.7990
		\\
		mutual\_info\_regression
		&
		10
		&
		0.7647
		& 
		0.6980
		\\
		pyHSICLasso
		&
		10
		& 
		0.7717
		&
		0.7004
		\\
		Boruta
		&
		8
		& 
		0.7669
		&
		0.7023
		\\
		minerva
		& 
		10
		&
		0.8799
		&
		0.8469
	\end{tabular}
}
\caption{Experiment 1.B - Accuracy of a gradient boosting model trained on the features selected by various methods}
\label{tab.experiment1Baccuracy}
\end{table}

